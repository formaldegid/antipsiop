{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEZ2RWrd6-vM",
    "outputId": "ce3b6fcc-73be-4783-ec6a-74930e2eba6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (2.9.1)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/hapikov.m@agtu.ru/.local/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, sentencepiece, safetensors, pyarrow, hf-xet, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets, evaluate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [evaluate]/12\u001b[0m [evaluate]ers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.4.1 dill-0.4.0 evaluate-0.4.6 hf-xet-1.2.0 huggingface-hub-0.36.0 multiprocess-0.70.18 pyarrow-22.0.0 safetensors-0.7.0 sentencepiece-0.2.1 tokenizers-0.22.1 transformers-4.57.3 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель работает на устройстве: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Модель работает на устройстве: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок обработки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_human</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bba9c2379d3841a282f7231e7b9c8505</td>\n",
       "      <td>Car-free cities have become a subject of incre...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bba9c2379d3841a282f7231e7b9c8505</td>\n",
       "      <td>. Cost Savings: Car ownership and maintenance ...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bba9c2379d3841a282f7231e7b9c8505</td>\n",
       "      <td>. Implement Carpooling and Ride-Sharing: Promo...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31a9bb04ab214c1c8c4b8bb9f8ee87e6</td>\n",
       "      <td>Car Free Cities Car-free cities, a concept gai...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31a9bb04ab214c1c8c4b8bb9f8ee87e6</td>\n",
       "      <td>. Parking lots and wide roads can be repurpose...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515139</th>\n",
       "      <td>fcc125800bba4ba1b0bf0b6f00ddc4ac</td>\n",
       "      <td>. 4. Идеи подарков: отдайтесь тренду впечатлен...</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515140</th>\n",
       "      <td>337db08bb3ab413093e7b756519128f0</td>\n",
       "      <td>Учёные сообщают о новых достижениях в области ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515141</th>\n",
       "      <td>0ffe9b7f11574eb584969a0c9f6c2ccd</td>\n",
       "      <td>Представьте себе идеальный день: вы просыпаете...</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515142</th>\n",
       "      <td>306168e83b444dc2827e02889a93494d</td>\n",
       "      <td>Москва, 10 мая 2024 г. — В условиях растущей с...</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515143</th>\n",
       "      <td>705feb5fbb8e48f8a5a44b0d118f1ef6</td>\n",
       "      <td>Москва, 12 сентября — Эксперты Всемирного фонд...</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2515144 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0        bba9c2379d3841a282f7231e7b9c8505   \n",
       "1        bba9c2379d3841a282f7231e7b9c8505   \n",
       "2        bba9c2379d3841a282f7231e7b9c8505   \n",
       "3        31a9bb04ab214c1c8c4b8bb9f8ee87e6   \n",
       "4        31a9bb04ab214c1c8c4b8bb9f8ee87e6   \n",
       "...                                   ...   \n",
       "2515139  fcc125800bba4ba1b0bf0b6f00ddc4ac   \n",
       "2515140  337db08bb3ab413093e7b756519128f0   \n",
       "2515141  0ffe9b7f11574eb584969a0c9f6c2ccd   \n",
       "2515142  306168e83b444dc2827e02889a93494d   \n",
       "2515143  705feb5fbb8e48f8a5a44b0d118f1ef6   \n",
       "\n",
       "                                                      text  is_human lang  \n",
       "0        Car-free cities have become a subject of incre...         0   en  \n",
       "1        . Cost Savings: Car ownership and maintenance ...         0   en  \n",
       "2        . Implement Carpooling and Ride-Sharing: Promo...         0   en  \n",
       "3        Car Free Cities Car-free cities, a concept gai...         0   en  \n",
       "4        . Parking lots and wide roads can be repurpose...         0   en  \n",
       "...                                                    ...       ...  ...  \n",
       "2515139  . 4. Идеи подарков: отдайтесь тренду впечатлен...         0   ru  \n",
       "2515140  Учёные сообщают о новых достижениях в области ...         0   ru  \n",
       "2515141  Представьте себе идеальный день: вы просыпаете...         0   ru  \n",
       "2515142  Москва, 10 мая 2024 г. — В условиях растущей с...         0   ru  \n",
       "2515143  Москва, 12 сентября — Эксперты Всемирного фонд...         0   ru  \n",
       "\n",
       "[2515144 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('AIgen_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Поиск всех записей на русском языке, оценка количества подходящих данных\n",
    "\n",
    "df['lang'].unique()\n",
    "len(df.loc[df['lang'] == 'ru'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Извлечение всех записей на русском языке подходящей длинны\n",
    "\n",
    "df_ru = df.loc[(df['lang'] == 'ru') & (df['text'].str.len() < 512)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36159\n",
      "36159\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Ребаланс по целевой метке, с целью одинакового распределния классов\n",
    "data = df_ru.drop(columns=['is_human'])\n",
    "rebalance = df_ru['is_human']\n",
    "\n",
    "rebalancer = RandomUnderSampler(sampling_strategy='majority', random_state = 2511)\n",
    "data_rsmpl, rebalance_rsmpl = rebalancer.fit_resample(data, rebalance)\n",
    "\n",
    "df_rebalanced = pd.DataFrame(data_rsmpl, columns=data.columns)\n",
    "df_rebalanced['is_human'] = rebalance_rsmpl\n",
    "\n",
    "print(len(df_rebalanced.loc[df['is_human'] == 1]))\n",
    "print(len(df_rebalanced.loc[df['is_human'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Изменение имён классов для дальнейшего удобства\n",
    "\n",
    "df_rebalanced['is_human'] = 1 - df_rebalanced['is_human']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2419491</th>\n",
       "      <td>В Руанде заработала система доставки крови при...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419496</th>\n",
       "      <td>После выхода фильма «Мушкетёры» актриса Мария ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419497</th>\n",
       "      <td>Она находится на стенде с чашкой чая; он застр...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419502</th>\n",
       "      <td>Королю Англии Генриху IV удалось собрать около...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419504</th>\n",
       "      <td>Только к концу позднего обеда, за тем, что Арс...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151466</th>\n",
       "      <td>Лучшие суши/роллы в городе! Заказываем уже бол...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263952</th>\n",
       "      <td>- А что будет, если прийти на выборы в майке с...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215061</th>\n",
       "      <td>Последний раз брала клубника подпорченная, вро...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226454</th>\n",
       "      <td>Вкусно, но не сравнить с помадкой от Красного ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464499</th>\n",
       "      <td>Теперь всё нормально. Не волнуйтесь. Вы можете...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72318 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  is_human\n",
       "2419491  В Руанде заработала система доставки крови при...         1\n",
       "2419496  После выхода фильма «Мушкетёры» актриса Мария ...         1\n",
       "2419497  Она находится на стенде с чашкой чая; он застр...         1\n",
       "2419502  Королю Англии Генриху IV удалось собрать около...         1\n",
       "2419504  Только к концу позднего обеда, за тем, что Арс...         1\n",
       "...                                                    ...       ...\n",
       "2151466  Лучшие суши/роллы в городе! Заказываем уже бол...         0\n",
       "2263952  - А что будет, если прийти на выборы в майке с...         0\n",
       "2215061  Последний раз брала клубника подпорченная, вро...         0\n",
       "2226454  Вкусно, но не сравнить с помадкой от Красного ...         0\n",
       "2464499  Теперь всё нормально. Не волнуйтесь. Вы можете...         0\n",
       "\n",
       "[72318 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Удаление лишних столбцов\n",
    "\n",
    "df = df_rebalanced.drop(columns=['lang','id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Исходная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653,
     "referenced_widgets": [
      "9a3aca9364c149d1ba6459160ea5d574",
      "28314141e87c4080b21f77cea4c79951",
      "fb566351da3047bdb59b28ee5a2c4ee3",
      "6a6566a7e77042f4bc8093b11e1da865",
      "ed10c75e394c440c89e2eb0cbc4ece61",
      "be9470d1ff4e46a3bc757248eb14cda9",
      "adbba2c88c3b4f4583c37c22ed528323",
      "673d88f72f1d4056b49cdd6de0fe2991",
      "92341578578d4a35a4906d30cf40f15c",
      "1161e5ba546f48ad926afaf30b2fdd3e",
      "7b79451913e74604901b97d1a4924e84",
      "3f5ff5bd37f444bfabbe652577730422",
      "35c6fd75114e479e8064e71b65eb614e",
      "d04c32ad5f44463a9a58abc485ac5dbd",
      "0c50a39c8bb5413ba57a83eaf122abe1",
      "f579d8a1c5294bd398374026c864a044",
      "d9599825798e44e487acb7526aba7fa8",
      "28e5d28c1a984526b02378d70d4c2e15",
      "38ee790f2bfa48259071464c01619e97",
      "158fbe29582b40aab460cd8a09fc7006",
      "f10b9ac62a544e9e96fb9370923a99de",
      "a27984088d7c4f98969e237355856ab0"
     ]
    },
    "id": "qQW-5CPw6oD1",
    "outputId": "93c1e3ed-934f-4bb2-d47b-5864f6f96599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'roberta' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизатор успешно загружен.\n",
      "Модель успешно загружена.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4369c299c84899a943d89b64591b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0539968d82e64267af9c1926b50b0c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9706/3232971321.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начинаем дообучение (fine-tuning) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180800' max='180800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180800/180800 10:10:32, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение завершено ---\n",
      "Лучшая модель сохранена в: ./xlm_roberta_ai_detector/best_model\n",
      "\n",
      "--- Загрузка обученной модели для инференса ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './xlm_roberta_ai_detector/best_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель работает на устройстве: cuda\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Текст: 'Я вчера ходил в кино с друзьями, фильм был неплохой.'\n",
      "Вероятность, что это ИИ: 1.0000 (ожидается низкая)\n",
      "--------------------\n",
      "Текст: 'Данная статья анализирует мультимодальные аспекты нейронных сетей.'\n",
      "Вероятность, что это ИИ: 1.0000 (ожидается высокая)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "LOAD_PATH = ''\n",
    "model_path = 'roberta'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Токенизатор успешно загружен.\")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "print(\"Модель успешно загружена.\")\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=2511)\n",
    "\n",
    "\n",
    "#Функция токенизации текста\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(\n",
    "        examples[\"text\"]\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"is_human\", \"labels\")\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "#Функция для вычисления метрик в процессе обучения\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "output_dir = \"./xlm_roberta_ai_detector\"\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "steps_to_save = 10*epochs\n",
    "\n",
    "#Инициализация параметров обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,      \n",
    "    learning_rate=2e-5,               \n",
    "    per_device_train_batch_size=batch_size,    \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,               \n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,               \n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",        \n",
    "    save_steps=steps_to_save,     \n",
    "    save_total_limit=1,           \n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_to_save\n",
    ")\n",
    "\n",
    "#Инициализация пайплайна обучения\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--- Начинаем дообучение (fine-tuning) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Обучение завершено ---\")\n",
    "\n",
    "\n",
    "best_model_path = f\"{output_dir}/best_model\"\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Лучшая модель сохранена в: {best_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Загрузка обученной модели для инференса ---\")\n",
    "trained_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model.to(device)\n",
    "print(f\"Модель работает на устройстве: {device}\")\n",
    "\n",
    "#Получение результатов классификации\n",
    "def get_ai_probability(text: str) -> float:\n",
    "\n",
    "    inputs = trained_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "    ai_prob = probabilities[0, 1].item()\n",
    "\n",
    "    return ai_prob\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Тестирование модели ---\")\n",
    "\n",
    "test_text_human = \"Я вчера ходил в кино с друзьями, фильм был неплохой.\"\n",
    "test_text_ai = \"Данная статья анализирует мультимодальные аспекты нейронных сетей.\"\n",
    "\n",
    "prob_1 = get_ai_probability(test_text_human)\n",
    "prob_2 = get_ai_probability(test_text_ai)\n",
    "\n",
    "print(f\"Текст: '{test_text_human}'\")\n",
    "print(f\"Вероятность, что это ИИ: {prob_1:.4f} (ожидается низкая)\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Текст: '{test_text_ai}'\")\n",
    "print(f\"Вероятность, что это ИИ: {prob_2:.4f} (ожидается высокая)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модель с меньшим количеством эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'roberta' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизатор успешно загружен.\n",
      "Модель успешно загружена.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289e84c9b3e542268df0c209201d0963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2e9b3a3268409ea8ce6fe0d6574f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_624011/2197723979.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начинаем дообучение (fine-tuning) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54240' max='54240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54240/54240 4:31:07, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.316735</td>\n",
       "      <td>0.891939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.258605</td>\n",
       "      <td>0.901410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.249421</td>\n",
       "      <td>0.907909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.232281</td>\n",
       "      <td>0.912472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.258539</td>\n",
       "      <td>0.904867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.240450</td>\n",
       "      <td>0.907978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.234037</td>\n",
       "      <td>0.917450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.226263</td>\n",
       "      <td>0.915584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.214611</td>\n",
       "      <td>0.921391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.239875</td>\n",
       "      <td>0.916067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.209091</td>\n",
       "      <td>0.920561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.225617</td>\n",
       "      <td>0.912887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.247973</td>\n",
       "      <td>0.917934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.219981</td>\n",
       "      <td>0.923396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.235827</td>\n",
       "      <td>0.920077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.220335</td>\n",
       "      <td>0.920907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.240972</td>\n",
       "      <td>0.917450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.204088</td>\n",
       "      <td>0.923949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.267950</td>\n",
       "      <td>0.910606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.227899</td>\n",
       "      <td>0.921668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.240909</td>\n",
       "      <td>0.923465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.246772</td>\n",
       "      <td>0.914339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.254916</td>\n",
       "      <td>0.923327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.266712</td>\n",
       "      <td>0.924779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.326990</td>\n",
       "      <td>0.914339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.299207</td>\n",
       "      <td>0.920423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.335441</td>\n",
       "      <td>0.909499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.338507</td>\n",
       "      <td>0.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.314532</td>\n",
       "      <td>0.917381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.276273</td>\n",
       "      <td>0.922152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.912403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.437467</td>\n",
       "      <td>0.912680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.337274</td>\n",
       "      <td>0.918418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.374656</td>\n",
       "      <td>0.918556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.372885</td>\n",
       "      <td>0.900166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.445960</td>\n",
       "      <td>0.902309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.449115</td>\n",
       "      <td>0.915722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.478038</td>\n",
       "      <td>0.921184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.514914</td>\n",
       "      <td>0.916897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.426197</td>\n",
       "      <td>0.919386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.439444</td>\n",
       "      <td>0.923949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.477304</td>\n",
       "      <td>0.904522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.522467</td>\n",
       "      <td>0.913025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.558747</td>\n",
       "      <td>0.913509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.516601</td>\n",
       "      <td>0.915584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.543389</td>\n",
       "      <td>0.916551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.497430</td>\n",
       "      <td>0.909569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.507053</td>\n",
       "      <td>0.921114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.490061</td>\n",
       "      <td>0.920285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.557028</td>\n",
       "      <td>0.919732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.505153</td>\n",
       "      <td>0.919386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.540515</td>\n",
       "      <td>0.911850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.582025</td>\n",
       "      <td>0.919663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.656089</td>\n",
       "      <td>0.908324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.544232</td>\n",
       "      <td>0.920976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.612804</td>\n",
       "      <td>0.907909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.602647</td>\n",
       "      <td>0.916344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.512439</td>\n",
       "      <td>0.915169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.625691</td>\n",
       "      <td>0.912611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.647983</td>\n",
       "      <td>0.908946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.632087</td>\n",
       "      <td>0.914823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.634936</td>\n",
       "      <td>0.918072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.646054</td>\n",
       "      <td>0.911919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.666479</td>\n",
       "      <td>0.919593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.643053</td>\n",
       "      <td>0.907356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.562189</td>\n",
       "      <td>0.915169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.585380</td>\n",
       "      <td>0.918487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.578356</td>\n",
       "      <td>0.919663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.662549</td>\n",
       "      <td>0.913440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.601642</td>\n",
       "      <td>0.916344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.602370</td>\n",
       "      <td>0.915169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.911712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.634663</td>\n",
       "      <td>0.919732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.562826</td>\n",
       "      <td>0.921322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.643903</td>\n",
       "      <td>0.922912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.625813</td>\n",
       "      <td>0.923396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.917381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.701762</td>\n",
       "      <td>0.920492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.691002</td>\n",
       "      <td>0.918764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.706526</td>\n",
       "      <td>0.913579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.708781</td>\n",
       "      <td>0.921045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.643586</td>\n",
       "      <td>0.922013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.697867</td>\n",
       "      <td>0.918764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.674719</td>\n",
       "      <td>0.914270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.606375</td>\n",
       "      <td>0.924710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.738509</td>\n",
       "      <td>0.911159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.720574</td>\n",
       "      <td>0.916482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>0.921184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.670488</td>\n",
       "      <td>0.913371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.743174</td>\n",
       "      <td>0.915929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.726606</td>\n",
       "      <td>0.915584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.720514</td>\n",
       "      <td>0.915169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.680871</td>\n",
       "      <td>0.913648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.795978</td>\n",
       "      <td>0.909845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.729244</td>\n",
       "      <td>0.916759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.778878</td>\n",
       "      <td>0.904038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.716824</td>\n",
       "      <td>0.913509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.781878</td>\n",
       "      <td>0.913648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.763022</td>\n",
       "      <td>0.921668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.734191</td>\n",
       "      <td>0.916621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.782273</td>\n",
       "      <td>0.911643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.749887</td>\n",
       "      <td>0.915998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.727554</td>\n",
       "      <td>0.919801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.743045</td>\n",
       "      <td>0.916897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.755694</td>\n",
       "      <td>0.920769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.726108</td>\n",
       "      <td>0.916482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.742958</td>\n",
       "      <td>0.908324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.745313</td>\n",
       "      <td>0.917865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.744574</td>\n",
       "      <td>0.912196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.706526</td>\n",
       "      <td>0.920907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.746430</td>\n",
       "      <td>0.916690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>0.908462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>0.920631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.714267</td>\n",
       "      <td>0.922152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.836243</td>\n",
       "      <td>0.909015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.763896</td>\n",
       "      <td>0.914270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>0.919870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.774333</td>\n",
       "      <td>0.912472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.816211</td>\n",
       "      <td>0.913509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.768300</td>\n",
       "      <td>0.924018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.788444</td>\n",
       "      <td>0.918626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.787730</td>\n",
       "      <td>0.916551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.765401</td>\n",
       "      <td>0.920077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.740804</td>\n",
       "      <td>0.919732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>0.913993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.739168</td>\n",
       "      <td>0.919524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.749904</td>\n",
       "      <td>0.920492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.769206</td>\n",
       "      <td>0.919801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.812218</td>\n",
       "      <td>0.920008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.917934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.735721</td>\n",
       "      <td>0.925539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.926853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39900</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.788179</td>\n",
       "      <td>0.923258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.774045</td>\n",
       "      <td>0.921322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.770751</td>\n",
       "      <td>0.923603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.783137</td>\n",
       "      <td>0.922497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41100</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.852790</td>\n",
       "      <td>0.916275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.860170</td>\n",
       "      <td>0.917450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41700</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.782935</td>\n",
       "      <td>0.919593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.807189</td>\n",
       "      <td>0.916621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42300</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.782713</td>\n",
       "      <td>0.922359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.825267</td>\n",
       "      <td>0.920077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42900</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.807681</td>\n",
       "      <td>0.921114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.776350</td>\n",
       "      <td>0.921322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.778896</td>\n",
       "      <td>0.923673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.805529</td>\n",
       "      <td>0.921529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.793258</td>\n",
       "      <td>0.923949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.823352</td>\n",
       "      <td>0.917865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44700</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.805381</td>\n",
       "      <td>0.920147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.789814</td>\n",
       "      <td>0.926231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45300</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.790051</td>\n",
       "      <td>0.923673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.795493</td>\n",
       "      <td>0.921806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45900</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.817890</td>\n",
       "      <td>0.920769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.874706</td>\n",
       "      <td>0.913786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.808972</td>\n",
       "      <td>0.925470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.923534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47100</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.818385</td>\n",
       "      <td>0.923534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.821122</td>\n",
       "      <td>0.919179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47700</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.818623</td>\n",
       "      <td>0.919248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.841290</td>\n",
       "      <td>0.919317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48300</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.922497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>0.919455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.847879</td>\n",
       "      <td>0.919040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.839241</td>\n",
       "      <td>0.920907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.853262</td>\n",
       "      <td>0.918971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.863775</td>\n",
       "      <td>0.917934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50100</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.855103</td>\n",
       "      <td>0.924157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.867974</td>\n",
       "      <td>0.922290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50700</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.865364</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.871834</td>\n",
       "      <td>0.922013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51300</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.875901</td>\n",
       "      <td>0.921045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51600</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.863467</td>\n",
       "      <td>0.922428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51900</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.851502</td>\n",
       "      <td>0.924640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.864996</td>\n",
       "      <td>0.921045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.855078</td>\n",
       "      <td>0.923119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.848366</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.850558</td>\n",
       "      <td>0.924087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849518</td>\n",
       "      <td>0.924640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53700</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.856294</td>\n",
       "      <td>0.922428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.856975</td>\n",
       "      <td>0.922082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение завершено ---\n",
      "Лучшая модель сохранена в: ./xlm_roberta_ai_detector_less_epoch/best_model\n",
      "\n",
      "--- Загрузка обученной модели для инференса ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './xlm_roberta_ai_detector_less_epoch/best_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель работает на устройстве: cuda\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Текст: 'Я вчера ходил в кино с друзьями, фильм был неплохой.'\n",
      "Вероятность, что это ИИ: 0.6899 (ожидается низкая)\n",
      "--------------------\n",
      "Текст: 'Данная статья анализирует мультимодальные аспекты нейронных сетей.'\n",
      "Вероятность, что это ИИ: 0.9316 (ожидается высокая)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "LOAD_PATH = ''\n",
    "model_path = 'roberta'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Токенизатор успешно загружен.\")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "print(\"Модель успешно загружена.\")\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=2511)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(\n",
    "        examples[\"text\"]\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"is_human\", \"labels\")\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "output_dir = \"./xlm_roberta_ai_detector_less_epoch\"\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "steps_to_save = 10*epochs\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,      \n",
    "    learning_rate=1e-5,               \n",
    "    per_device_train_batch_size=batch_size,    \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,               \n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,               \n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",        \n",
    "    save_steps=steps_to_save,     \n",
    "    save_total_limit=1,           \n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_to_save\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--- Начинаем дообучение (fine-tuning) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Обучение завершено ---\")\n",
    "\n",
    "\n",
    "best_model_path = f\"{output_dir}/best_model\"\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Лучшая модель сохранена в: {best_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Загрузка обученной модели для инференса ---\")\n",
    "trained_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model.to(device)\n",
    "print(f\"Модель работает на устройстве: {device}\")\n",
    "\n",
    "\n",
    "def get_ai_probability(text: str) -> float:\n",
    "\n",
    "    inputs = trained_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "    ai_prob = probabilities[0, 1].item()\n",
    "\n",
    "    return ai_prob\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Тестирование модели ---\")\n",
    "\n",
    "test_text_human = \"Я вчера ходил в кино с друзьями, фильм был неплохой.\"\n",
    "test_text_ai = \"Данная статья анализирует мультимодальные аспекты нейронных сетей.\"\n",
    "\n",
    "prob_1 = get_ai_probability(test_text_human)\n",
    "prob_2 = get_ai_probability(test_text_ai)\n",
    "\n",
    "print(f\"Текст: '{test_text_human}'\")\n",
    "print(f\"Вероятность, что это ИИ: {prob_1:.4f} (ожидается низкая)\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Текст: '{test_text_ai}'\")\n",
    "print(f\"Вероятность, что это ИИ: {prob_2:.4f} (ожидается высокая)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучение только классифицирующей головы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'roberta' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизатор успешно загружен.\n",
      "Модель успешно загружена.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac79e0f673f432d9676894f4c51c521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc0a9e3f04e470b9f147962f558919e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_624011/1991360434.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начинаем дообучение (fine-tuning) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180800' max='180800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180800/180800 4:25:06, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.629594</td>\n",
       "      <td>0.820174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.565389</td>\n",
       "      <td>0.862832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>0.866012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.459415</td>\n",
       "      <td>0.868847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.426575</td>\n",
       "      <td>0.868501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.404122</td>\n",
       "      <td>0.869607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.387911</td>\n",
       "      <td>0.870506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.376178</td>\n",
       "      <td>0.868985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.366453</td>\n",
       "      <td>0.871958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.359685</td>\n",
       "      <td>0.871197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.354973</td>\n",
       "      <td>0.874170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.350687</td>\n",
       "      <td>0.872580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.346395</td>\n",
       "      <td>0.873479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.343692</td>\n",
       "      <td>0.873133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.340339</td>\n",
       "      <td>0.873410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.337407</td>\n",
       "      <td>0.875830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>0.335293</td>\n",
       "      <td>0.874239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.333158</td>\n",
       "      <td>0.876037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.332201</td>\n",
       "      <td>0.875138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.330061</td>\n",
       "      <td>0.877627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.328525</td>\n",
       "      <td>0.877835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.327488</td>\n",
       "      <td>0.876798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>0.327652</td>\n",
       "      <td>0.878457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.325720</td>\n",
       "      <td>0.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.323855</td>\n",
       "      <td>0.879978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.323028</td>\n",
       "      <td>0.879079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.322512</td>\n",
       "      <td>0.880393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.321292</td>\n",
       "      <td>0.880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.321915</td>\n",
       "      <td>0.880393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.319966</td>\n",
       "      <td>0.879909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.319576</td>\n",
       "      <td>0.880324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.319044</td>\n",
       "      <td>0.880462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.363800</td>\n",
       "      <td>0.318280</td>\n",
       "      <td>0.880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.881845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.319081</td>\n",
       "      <td>0.881430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.317136</td>\n",
       "      <td>0.880877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.317166</td>\n",
       "      <td>0.882329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.316054</td>\n",
       "      <td>0.880877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.315766</td>\n",
       "      <td>0.881153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.315317</td>\n",
       "      <td>0.881291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.315014</td>\n",
       "      <td>0.881291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.364800</td>\n",
       "      <td>0.314891</td>\n",
       "      <td>0.881775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.364900</td>\n",
       "      <td>0.314568</td>\n",
       "      <td>0.881845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.314817</td>\n",
       "      <td>0.882605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.314328</td>\n",
       "      <td>0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.352800</td>\n",
       "      <td>0.313596</td>\n",
       "      <td>0.882743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.366500</td>\n",
       "      <td>0.313181</td>\n",
       "      <td>0.881983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.313113</td>\n",
       "      <td>0.882190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.312932</td>\n",
       "      <td>0.882743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.314539</td>\n",
       "      <td>0.882467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.312343</td>\n",
       "      <td>0.881983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.312083</td>\n",
       "      <td>0.882259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.311932</td>\n",
       "      <td>0.883089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.311576</td>\n",
       "      <td>0.883089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.311623</td>\n",
       "      <td>0.883158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.312051</td>\n",
       "      <td>0.883850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.310949</td>\n",
       "      <td>0.883296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.360600</td>\n",
       "      <td>0.310921</td>\n",
       "      <td>0.883227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.310739</td>\n",
       "      <td>0.883642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>0.310716</td>\n",
       "      <td>0.883642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.330900</td>\n",
       "      <td>0.312855</td>\n",
       "      <td>0.883366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.310373</td>\n",
       "      <td>0.883919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.310724</td>\n",
       "      <td>0.883850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.363100</td>\n",
       "      <td>0.310025</td>\n",
       "      <td>0.883020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.353200</td>\n",
       "      <td>0.309754</td>\n",
       "      <td>0.883366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.357100</td>\n",
       "      <td>0.310215</td>\n",
       "      <td>0.881775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.309469</td>\n",
       "      <td>0.882052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.309164</td>\n",
       "      <td>0.883642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>0.309122</td>\n",
       "      <td>0.884057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.308911</td>\n",
       "      <td>0.883573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.358600</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>0.884403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.310307</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.308531</td>\n",
       "      <td>0.883227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.308833</td>\n",
       "      <td>0.883988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.308609</td>\n",
       "      <td>0.883850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.308907</td>\n",
       "      <td>0.884334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.308613</td>\n",
       "      <td>0.884264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.308283</td>\n",
       "      <td>0.884195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>0.308634</td>\n",
       "      <td>0.884264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.308013</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.308010</td>\n",
       "      <td>0.884748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>0.307863</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>0.307732</td>\n",
       "      <td>0.884887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>0.307896</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.342500</td>\n",
       "      <td>0.310565</td>\n",
       "      <td>0.885232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.309672</td>\n",
       "      <td>0.884887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.307766</td>\n",
       "      <td>0.884541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.307382</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.307110</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.307561</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.307653</td>\n",
       "      <td>0.884541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.368500</td>\n",
       "      <td>0.307091</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.308587</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.307641</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.306912</td>\n",
       "      <td>0.885232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.307093</td>\n",
       "      <td>0.885025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.307451</td>\n",
       "      <td>0.884748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.306930</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.307022</td>\n",
       "      <td>0.884748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.306636</td>\n",
       "      <td>0.885371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.306440</td>\n",
       "      <td>0.885025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.306972</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>0.306320</td>\n",
       "      <td>0.884403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.306974</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.306320</td>\n",
       "      <td>0.884264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.307262</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.306545</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.306027</td>\n",
       "      <td>0.884264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.360400</td>\n",
       "      <td>0.306303</td>\n",
       "      <td>0.885025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.306036</td>\n",
       "      <td>0.885509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.884887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.306354</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.305912</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.306438</td>\n",
       "      <td>0.884334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.306399</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.354500</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.884956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.305963</td>\n",
       "      <td>0.885025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.306970</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.306779</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.305732</td>\n",
       "      <td>0.885785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>0.305981</td>\n",
       "      <td>0.885232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.305813</td>\n",
       "      <td>0.884334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.306305</td>\n",
       "      <td>0.884956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.305781</td>\n",
       "      <td>0.884956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.305913</td>\n",
       "      <td>0.884748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.358800</td>\n",
       "      <td>0.305853</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.305487</td>\n",
       "      <td>0.885647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.306012</td>\n",
       "      <td>0.884403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>0.306286</td>\n",
       "      <td>0.885025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.305695</td>\n",
       "      <td>0.884679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.357100</td>\n",
       "      <td>0.305812</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>0.305385</td>\n",
       "      <td>0.885371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.305304</td>\n",
       "      <td>0.884403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.305675</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.305601</td>\n",
       "      <td>0.884817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.305414</td>\n",
       "      <td>0.885371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>0.305237</td>\n",
       "      <td>0.885440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.305226</td>\n",
       "      <td>0.883780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.305546</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.305092</td>\n",
       "      <td>0.885509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.351600</td>\n",
       "      <td>0.305220</td>\n",
       "      <td>0.884057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.305236</td>\n",
       "      <td>0.885578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.305120</td>\n",
       "      <td>0.885647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.305522</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.305137</td>\n",
       "      <td>0.885785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.305640</td>\n",
       "      <td>0.884887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.305340</td>\n",
       "      <td>0.884403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.305577</td>\n",
       "      <td>0.884541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.305312</td>\n",
       "      <td>0.884334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.351600</td>\n",
       "      <td>0.305101</td>\n",
       "      <td>0.885232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.305035</td>\n",
       "      <td>0.885716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.305384</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.305066</td>\n",
       "      <td>0.885301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.305130</td>\n",
       "      <td>0.884748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.305048</td>\n",
       "      <td>0.885578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.305519</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.304844</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.349900</td>\n",
       "      <td>0.305205</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>0.363400</td>\n",
       "      <td>0.304820</td>\n",
       "      <td>0.885785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.304784</td>\n",
       "      <td>0.885993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.305325</td>\n",
       "      <td>0.884541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.304974</td>\n",
       "      <td>0.885509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>0.305070</td>\n",
       "      <td>0.885094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>0.356200</td>\n",
       "      <td>0.304929</td>\n",
       "      <td>0.885509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.304859</td>\n",
       "      <td>0.885855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>0.305217</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.305069</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.304797</td>\n",
       "      <td>0.885993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.305069</td>\n",
       "      <td>0.884472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.305020</td>\n",
       "      <td>0.884956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.304941</td>\n",
       "      <td>0.885163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.304954</td>\n",
       "      <td>0.885301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.304825</td>\n",
       "      <td>0.885785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.304898</td>\n",
       "      <td>0.885647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.304969</td>\n",
       "      <td>0.885232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.305059</td>\n",
       "      <td>0.884610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.305004</td>\n",
       "      <td>0.885163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.885163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение завершено ---\n",
      "Лучшая модель сохранена в: ./xlm_roberta_ai_detector_only_head/best_model\n",
      "\n",
      "--- Загрузка обученной модели для инференса ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './xlm_roberta_ai_detector_only_head/best_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель работает на устройстве: cuda\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Текст: 'Я вчера ходил в кино с друзьями, фильм был неплохой.'\n",
      "Вероятность, что это ИИ: 0.3337 (ожидается низкая)\n",
      "--------------------\n",
      "Текст: 'Данная статья анализирует мультимодальные аспекты нейронных сетей.'\n",
      "Вероятность, что это ИИ: 0.9645 (ожидается высокая)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "LOAD_PATH = ''\n",
    "model_path = 'roberta'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Токенизатор успешно загружен.\")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Модель успешно загружена.\")\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=2511)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(\n",
    "        examples[\"text\"]\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"is_human\", \"labels\")\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "output_dir = \"./xlm_roberta_ai_detector_only_head\"\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "steps_to_save = 10*epochs\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,      \n",
    "    learning_rate=1e-5,               \n",
    "    per_device_train_batch_size=batch_size,    \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,               \n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,               \n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",        \n",
    "    save_steps=steps_to_save,     \n",
    "    save_total_limit=1,           \n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_to_save\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--- Начинаем дообучение (fine-tuning) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Обучение завершено ---\")\n",
    "\n",
    "\n",
    "best_model_path = f\"{output_dir}/best_model\"\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Лучшая модель сохранена в: {best_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Загрузка обученной модели для инференса ---\")\n",
    "trained_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model.to(device)\n",
    "print(f\"Модель работает на устройстве: {device}\")\n",
    "\n",
    "\n",
    "def get_ai_probability(text: str) -> float:\n",
    "\n",
    "    inputs = trained_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "    ai_prob = probabilities[0, 1].item()\n",
    "\n",
    "    return ai_prob\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Тестирование модели ---\")\n",
    "\n",
    "test_text_human = \"Я вчера ходил в кино с друзьями, фильм был неплохой.\"\n",
    "test_text_ai = \"Данная статья анализирует мультимодальные аспекты нейронных сетей.\"\n",
    "\n",
    "prob_1 = get_ai_probability(test_text_human)\n",
    "prob_2 = get_ai_probability(test_text_ai)\n",
    "\n",
    "print(f\"Текст: '{test_text_human}'\")\n",
    "print(f\"Вероятность, что это ИИ: {prob_1:.4f} (ожидается низкая)\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Текст: '{test_text_ai}'\")\n",
    "print(f\"Вероятность, что это ИИ: {prob_2:.4f} (ожидается высокая)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (CUDA-Env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c50a39c8bb5413ba57a83eaf122abe1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f10b9ac62a544e9e96fb9370923a99de",
      "placeholder": "​",
      "style": "IPY_MODEL_a27984088d7c4f98969e237355856ab0",
      "value": " 2/2 [00:00&lt;00:00, 28.57 examples/s]"
     }
    },
    "1161e5ba546f48ad926afaf30b2fdd3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "158fbe29582b40aab460cd8a09fc7006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "28314141e87c4080b21f77cea4c79951": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be9470d1ff4e46a3bc757248eb14cda9",
      "placeholder": "​",
      "style": "IPY_MODEL_adbba2c88c3b4f4583c37c22ed528323",
      "value": "Map: 100%"
     }
    },
    "28e5d28c1a984526b02378d70d4c2e15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35c6fd75114e479e8064e71b65eb614e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9599825798e44e487acb7526aba7fa8",
      "placeholder": "​",
      "style": "IPY_MODEL_28e5d28c1a984526b02378d70d4c2e15",
      "value": "Map: 100%"
     }
    },
    "38ee790f2bfa48259071464c01619e97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f5ff5bd37f444bfabbe652577730422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35c6fd75114e479e8064e71b65eb614e",
       "IPY_MODEL_d04c32ad5f44463a9a58abc485ac5dbd",
       "IPY_MODEL_0c50a39c8bb5413ba57a83eaf122abe1"
      ],
      "layout": "IPY_MODEL_f579d8a1c5294bd398374026c864a044"
     }
    },
    "673d88f72f1d4056b49cdd6de0fe2991": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a6566a7e77042f4bc8093b11e1da865": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1161e5ba546f48ad926afaf30b2fdd3e",
      "placeholder": "​",
      "style": "IPY_MODEL_7b79451913e74604901b97d1a4924e84",
      "value": " 6/6 [00:00&lt;00:00, 67.91 examples/s]"
     }
    },
    "7b79451913e74604901b97d1a4924e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92341578578d4a35a4906d30cf40f15c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a3aca9364c149d1ba6459160ea5d574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28314141e87c4080b21f77cea4c79951",
       "IPY_MODEL_fb566351da3047bdb59b28ee5a2c4ee3",
       "IPY_MODEL_6a6566a7e77042f4bc8093b11e1da865"
      ],
      "layout": "IPY_MODEL_ed10c75e394c440c89e2eb0cbc4ece61"
     }
    },
    "a27984088d7c4f98969e237355856ab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "adbba2c88c3b4f4583c37c22ed528323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be9470d1ff4e46a3bc757248eb14cda9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d04c32ad5f44463a9a58abc485ac5dbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38ee790f2bfa48259071464c01619e97",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_158fbe29582b40aab460cd8a09fc7006",
      "value": 2
     }
    },
    "d9599825798e44e487acb7526aba7fa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed10c75e394c440c89e2eb0cbc4ece61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f10b9ac62a544e9e96fb9370923a99de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f579d8a1c5294bd398374026c864a044": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb566351da3047bdb59b28ee5a2c4ee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_673d88f72f1d4056b49cdd6de0fe2991",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92341578578d4a35a4906d30cf40f15c",
      "value": 6
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
